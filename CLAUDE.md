# Engram

Flutter app that reads an Outline wiki, uses Claude API to extract a knowledge graph (concepts + relationships + quiz items), and teaches it back via SM-2 spaced repetition. Visual mind map that "lights up" as you learn.

## Architecture
- **Models**: Immutable data classes with `fromJson`/`toJson` and `withXxx()` update methods
- **SM-2 Engine**: Pure function, no class state. Scheduling state lives on `QuizItem`
- **Graph Analyzer**: Dependency-aware concept unlocking, topological sort, cycle detection
- **Storage**: `GraphStore` — single JSON file via `path_provider`; `SettingsRepository` — API keys via `shared_preferences`
- **State Management**: Riverpod (manual, no codegen) — 8 providers, `Notifier`/`AsyncNotifier` classes
- **Mind Map**: `flutter_graph_view` force-directed graph, nodes colored by mastery (grey→red→amber→green)
- **Services**: `OutlineClient` (HTTP), `ExtractionService` (Claude API via `anthropic_sdk_dart`)

## Screens
- **Dashboard**: Stats cards, mastery bar, mind map visualization
- **Quiz**: Question → Reveal → Rate (0-5) → Session summary
- **Ingest**: Collection picker → per-document extraction progress
- **Settings**: API key configuration (Outline URL/key, Anthropic key)

## Development
```bash
flutter pub get
flutter analyze
flutter test
flutter run -d macos
```

## Testing
Tests mirror lib/src/ structure. Use `mocktail` for mocking HTTP clients. Widget tests override providers with `_PreloadedGraphNotifier` to avoid async I/O. Use `pump()` instead of `pumpAndSettle()` when `FlutterGraphWidget` is rendered (force-directed animation never settles).

## Open Investigations

When starting a new session, prompt the user about these:

### Graph-based state management vs Riverpod
Currently the entire `KnowledgeGraph` is a single immutable blob in one `AsyncNotifierProvider`. Updating one quiz item replaces the whole graph and rebuilds all watchers. This is fine at 20-100 concepts but may hurt at 500+. Investigate: splitting into `family` providers (one per concept/quiz-item) or adopting an atom-based reactive model for surgical rebuilds. Key trigger: when mind map re-renders become visibly sluggish.

### Custom force-directed canvas vs flutter_graph_view
`flutter_graph_view` got us to "working mind map" fast but has friction: untyped `Map`-based API forced a `GraphDataMapper` translation layer, `List<String?>` type mismatch was a runtime error, continuous animation breaks `pumpAndSettle` in tests, and we can't animate node color transitions. Investigate: building a custom `CustomPainter` (~400 lines) with our own `ForceDirectedLayout` class that works directly with `Concept`/`Relationship` models. Would enable glow effects, animated mastery transitions, and proper test compatibility.

### Per-collection or wiki-wide graph?
Currently ingestion is per-collection but the graph merges across collections — effectively wiki-wide. Should users be able to maintain separate graphs per collection? Or is the cross-collection merge the right default? Matters for teams with large wikis where concepts from unrelated collections might collide.

### Manual quiz item creation
All quiz items are auto-generated by Claude. Should users be able to add their own? Would help fill gaps where the LLM misses nuance or where a user wants to test something specific. Could be a simple "Add card" button on the quiz screen or a concept detail view.

### Multiplayer / team mode
Should teammates see each other's mastery? A shared graph where you can see "Alice has mastered Kubernetes, Bob hasn't reviewed Docker yet" could drive team learning culture. Requires a shared backend (Firestore?) and raises privacy questions.

### Feeding mastery back into Outline
Could concept mastery data flow back into the wiki? e.g., tagging pages as "well-understood by 80% of the team" or surfacing knowledge gaps ("nobody has mastered the CI/CD section"). Would need Outline API write access and a clear UX for what "team mastery" means.
